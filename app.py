import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
import PyPDF2
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
import warnings
from pythainlp.tokenize import word_tokenize, sent_tokenize
from pythainlp.util import normalize
import plotly.graph_objects as go
import plotly.express as px

warnings.filterwarnings('ignore')

# Import our custom modules
from rag_system import ExperimentalThaiTextProcessor, ExperimentalVectorDatabase, ExperimentalCAMTLlamaRAG
from utils import load_css, initialize_session_state

# Page configuration
st.set_page_config(
    page_title="Finddy - Financial Knowledge Assistant",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Load custom CSS
load_css()

# Initialize session state
initialize_session_state()

# Sidebar navigation
with st.sidebar:
    st.image("https://via.placeholder.com/150x50/8B7355/FFFFFF?text=Finddy", width=150)
    st.markdown("---")

    page = st.radio(
        "‡πÄ‡∏°‡∏ô‡∏π‡∏´‡∏•‡∏±‡∏Å",
        ["üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ", "üìä ‡πÅ‡∏ú‡∏á‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°", "üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"],
        index=0
    )

    st.markdown("---")
    st.markdown("### üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ", st.session_state.get('searches_today', 0))
    with col2:
        st.metric("‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", st.session_state.get('total_docs', 0))


# Initialize RAG components
@st.cache_resource
def initialize_rag_system():
    """Initialize the RAG system components"""
    processor = ExperimentalThaiTextProcessor()
    vector_db = ExperimentalVectorDatabase("streamlit_app")

    # Load existing database if available
    if os.path.exists("data/vector_db.json"):
        vector_db.load()

    rag_system = ExperimentalCAMTLlamaRAG(vector_db)
    return processor, vector_db, rag_system


processor, vector_db, rag_system = initialize_rag_system()

# Main content based on selected page
if page == "üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ":
    st.title("üîç Finddy - ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô")
    st.markdown("Financial Knowledge Assistant")

    # Search interface
    col1, col2, col3 = st.columns([3, 1, 1])

    with col1:
        query = st.text_input(
            "‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì",
            placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏°‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏î‡∏¢‡πà‡∏≠‡∏¢, ‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î...",
            key="search_query"
        )

    with col2:
        departments = ["‡∏ó‡∏∏‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å", "‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Ç‡∏≤‡∏¢", "‡πÑ‡∏≠‡∏ó‡∏µ"]
        selected_dept = st.selectbox("‡πÅ‡∏ú‡∏ô‡∏Å", departments, key="dept_filter")

    with col3:
        doc_types = ["‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ô‡∏ß‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠", "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°",
                     "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®"]
        selected_type = st.selectbox("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", doc_types, key="type_filter")

    # Quick search tags
    st.markdown("### ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°")
    col1, col2, col3, col4, col5 = st.columns(5)

    quick_searches = [
        ("üí∞ ‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£", "‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£ 50000"),
        ("üíµ ‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏î‡∏¢‡πà‡∏≠‡∏¢", "‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏î‡∏¢‡πà‡∏≠‡∏¢ 3000"),
        ("üìä ‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢", "‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢ 0.5%"),
        ("üíª Smart Office", "Smart Office"),
        ("üßæ ‡∏Ñ‡πà‡∏≤‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á", "‡∏Ñ‡πà‡∏≤‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á")
    ]

    for col, (label, search_text) in zip([col1, col2, col3, col4, col5], quick_searches):
        with col:
            if st.button(label, use_container_width=True):
                st.session_state['search_query'] = search_text
                st.rerun()

    # Search button
    if st.button("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", type="primary", use_container_width=True):
        if query:
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤..."):
                # Update search count
                st.session_state['searches_today'] = st.session_state.get('searches_today', 0) + 1

                # Get available sources
                available_sources = vector_db.get_available_sources()

                if not available_sources:
                    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô")
                else:
                    # Search across all sources
                    all_results = []
                    for source in available_sources:
                        results = vector_db.search_by_source_with_metadata(
                            query, source, k=3
                        )
                        if results:
                            # Group results by source
                            source_result = {
                                'source': source,
                                'title': results[0]['title'],
                                'department': results[0]['department'],
                                'type': results[0]['type'],
                                'chunks': results,
                                'avg_similarity': np.mean([r['similarity'] for r in results])
                            }
                            all_results.append(source_result)

                    # Sort by average similarity
                    all_results.sort(key=lambda x: x['avg_similarity'], reverse=True)

                    # Apply filters
                    if selected_dept != "‡∏ó‡∏∏‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å":
                        all_results = [r for r in all_results if r['department'] == selected_dept]

                    if selected_type != "‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó":
                        all_results = [r for r in all_results if r['type'] == selected_type]

                    # Display results
                    if all_results:
                        st.markdown("### üìö ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")

                        for idx, result in enumerate(all_results[:5]):  # Show top 5
                            with st.expander(f"{idx + 1}. {result['title']}", expanded=(idx == 0)):
                                col1, col2 = st.columns([3, 1])

                                with col1:
                                    st.markdown(f"**‡πÅ‡∏ú‡∏ô‡∏Å:** {result['department']}")
                                    st.markdown(f"**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:** {result['type']}")
                                    st.markdown(f"**‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô:** {result['avg_similarity'] * 100:.1f}%")

                                with col2:
                                    if st.button(f"‡∏î‡∏π‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", key=f"view_{idx}"):
                                        # Get answer from RAG
                                        answer_result = rag_system.ask_with_enhanced_retrieval(
                                            query, result['source']
                                        )
                                        st.session_state['current_answer'] = answer_result
                                        st.session_state['show_answer'] = True

                                # Show preview of relevant chunks
                                st.markdown("**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**")
                                for chunk in result['chunks'][:2]:
                                    st.info(chunk['text'][:200] + "...")
                    else:
                        st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì")

    # Display answer if available
    if st.session_state.get('show_answer', False) and 'current_answer' in st.session_state:
        st.markdown("---")
        st.markdown("### üí° ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")

        answer_data = st.session_state['current_answer']

        # Answer card
        with st.container():
            st.markdown(f"**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {answer_data['query']}")
            st.markdown(f"**‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:** {answer_data.get('selected_title', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")

            # Display answer
            st.markdown("**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**")
            st.success(answer_data['answer'])

            # Metadata
            if answer_data.get('metadata'):
                with st.expander("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"):
                    meta = answer_data['metadata']
                    st.markdown(f"- **‡πÅ‡∏ú‡∏ô‡∏Å:** {meta.get('department', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
                    st.markdown(f"- **‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:** {meta.get('type', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
                    st.markdown(f"- **‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** {', '.join(meta.get('keywords', []))}")

            # Feedback
            col1, col2, col3 = st.columns([1, 1, 3])
            with col1:
                if st.button("üëç ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"):
                    st.session_state['feedback_submitted'] = True
                    st.success("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô!")
            with col2:
                if st.button("üëé ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"):
                    st.session_state['feedback_submitted'] = True
                    st.info("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")

elif page == "üìä ‡πÅ‡∏ú‡∏á‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°":
    st.title("üìä ‡πÅ‡∏ú‡∏á‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°")

    # Dashboard metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            "üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
            st.session_state.get('total_docs', 0),
            "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"
        )

    with col2:
        st.metric(
            "üîç ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ",
            st.session_state.get('searches_today', 0),
            "+12% ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô"
        )

    with col3:
        st.metric(
            "üòä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à",
            "92%",
            "+3% ‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß"
        )

    with col4:
        st.metric(
            "‚ö° ‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á",
            "187ms",
            "-23ms ‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß"
        )

    # Charts
    st.markdown("### üìà ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•")

    col1, col2 = st.columns(2)

    with col1:
        # Search trends
        dates = pd.date_range(end=datetime.now(), periods=7).to_list()
        searches = np.random.randint(200, 400, size=7)

        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=dates,
            y=searches,
            mode='lines+markers',
            name='‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤',
            line=dict(color='#8B7355', width=3),
            marker=dict(size=8)
        ))
        fig.update_layout(
            title="‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ 7 ‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á",
            xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
            yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # Document distribution
        doc_types = ["‡πÅ‡∏ô‡∏ß‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"]
        doc_counts = [7, 5, 4, 3, 2]

        fig = go.Figure(data=[go.Pie(
            labels=doc_types,
            values=doc_counts,
            hole=.3,
            marker_colors=['#8B7355', '#A68B6F', '#C4A57B', '#E2C488', '#F0DBA5']
        )])
        fig.update_layout(
            title="‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)

    # Document table
    st.markdown("### üìö ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

    # Get documents from vector database
    if hasattr(vector_db, 'data') and vector_db.data:
        # Create document list from vector database
        documents = []
        seen_sources = set()

        for doc in vector_db.data:
            if doc['source'] not in seen_sources:
                seen_sources.add(doc['source'])
                documents.append({
                    '‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£': doc['title'],
                    '‡πÅ‡∏ú‡∏ô‡∏Å': doc['department'],
                    '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó': doc['type'],
                    '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': doc['date'],
                    '‡πÑ‡∏ü‡∏•‡πå': doc['source']
                })

        if documents:
            df = pd.DataFrame(documents)
            st.dataframe(df, use_container_width=True)
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")
    else:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")

elif page == "üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£":
    st.title("üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

    # File upload
    uploaded_file = st.file_uploader(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF",
        type=['pdf'],
        help="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå PDF"
    )

    if uploaded_file:
        # Document metadata form
        with st.form("upload_form"):
            st.markdown("### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

            col1, col2 = st.columns(2)

            with col1:
                title = st.text_input(
                    "‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
                    value=uploaded_file.name.replace('.pdf', ''),
                    help="‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
                )

                department = st.selectbox(
                    "‡πÅ‡∏ú‡∏ô‡∏Å",
                    ["‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Ç‡∏≤‡∏¢", "‡πÑ‡∏≠‡∏ó‡∏µ"]
                )

                doc_type = st.selectbox(
                    "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó",
                    ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ô‡∏ß‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠", "‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®"]
                )

            with col2:
                doc_date = st.date_input(
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                    value=datetime.now()
                )

                keywords = st.text_input(
                    "‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç",
                    placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ, ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£, ‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢",
                    help="‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ"
                )

                doc_number = st.text_input(
                    "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
                    placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏ß 1234/2567"
                )

            # Submit button
            submitted = st.form_submit_button("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", type="primary", use_container_width=True)

            if submitted:
                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£..."):
                    try:
                        # Extract text from PDF
                        pdf_reader = PyPDF2.PdfReader(uploaded_file)
                        full_text = ""

                        for page_num in range(len(pdf_reader.pages)):
                            page = pdf_reader.pages[page_num]
                            page_text = page.extract_text()
                            full_text += page_text + "\n"

                        # Normalize text
                        full_text = processor.normalize_thai_text(full_text)

                        # Create document metadata
                        metadata = {
                            'title': title,
                            'department': department,
                            'type': doc_type,
                            'date': doc_date.strftime("%Y-%m-%d"),
                            'keywords': [k.strip() for k in keywords.split(',') if k.strip()],
                            'doc_number': doc_number,
                            'filename': uploaded_file.name
                        }

                        # Chunk text using whitespace method with 300 chunk size
                        chunks = processor.chunk_by_whitespace(
                            full_text,
                            title,
                            chunk_size=300,
                            overlap_ratio=0.2
                        )

                        # Create documents for vector database
                        documents = []
                        for i, chunk_data in enumerate(chunks):
                            doc = {
                                'id': f"{uploaded_file.name}_{i}",
                                'source': uploaded_file.name,
                                'text': chunk_data['text'],
                                'title': metadata['title'],
                                'department': metadata['department'],
                                'type': metadata['type'],
                                'date': metadata['date'],
                                'keywords': metadata['keywords'],
                                'doc_number': metadata['doc_number'],
                                'chunk_index': i,
                                'total_chunks': len(chunks),
                                'chunk_method': 'whitespace',
                                'chunk_size': 300
                            }
                            documents.append(doc)

                        # Embed and add to vector database
                        vector_db.embed_documents(documents)
                        vector_db.save()

                        # Update document count
                        st.session_state['total_docs'] = st.session_state.get('total_docs', 0) + 1

                        st.success(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏™‡∏£‡πâ‡∏≤‡∏á {len(chunks)} chunks")

                        # Clear form
                        st.rerun()

                    except Exception as e:
                        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <p>Finddy - Financial Knowledge Assistant ¬© 2024</p>
        <p>Powered by RAG System with Thai Text Processing</p>
    </div>
    """,
    unsafe_allow_html=True
)